{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:20:24.742771Z",
     "start_time": "2025-05-16T01:20:24.740233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os # For path.basename and cpu_count\n",
    "import tensorflow as tf # Ensure tf is imported for type hints and operations\n",
    "from tensorflow.keras.models import load_model # For loading model in worker\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm # For progress bar"
   ],
   "id": "46d230f9698cf0de",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:29:33.923620Z",
     "start_time": "2025-05-16T01:29:33.920509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combined_mse_cosine_loss(y_true, y_pred):\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    y_true_norm = tf.nn.l2_normalize(y_true, axis=1)\n",
    "    y_pred_norm = tf.nn.l2_normalize(y_pred, axis=1)\n",
    "    cosine_loss = 1 - tf.reduce_mean(tf.reduce_sum(y_true_norm * y_pred_norm, axis=1))\n",
    "    return mse + 0.3 * cosine_loss"
   ],
   "id": "22c7d27ff892b3c7",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:29:34.794400Z",
     "start_time": "2025-05-16T01:29:34.791561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"models/resnet50.keras\"\n",
    "new_spectrogram_path = \"spectrogram/test_data/Bach0_30_spectrogram_win_length=2048_hop_length=512_n_fft=2048.png\"\n",
    "FEATURE_NAMES = [\n",
    "    \"acousticness\", \"instrumentalness\", \"liveness\", \"speechiness\",\n",
    "    \"danceability\", \"energy\", \"tempo\", \"valence\"\n",
    "]\n",
    "\n",
    "if len(FEATURE_NAMES) != 8:\n",
    "    raise ValueError(\"FEATURE_NAMES list must contain exactly 8 names.\")"
   ],
   "id": "c0175dad3af66b3b",
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T01:29:38.579289Z",
     "start_time": "2025-05-16T01:29:36.504907Z"
    }
   },
   "source": [
    "# 1. Load the trained model with the custom loss function\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "# Ensure model_path is a string or Path object correctly pointing to your model\n",
    "# from pathlib import Path # if you want to use Path objects\n",
    "# model_path_obj = Path(model_path)\n",
    "# if not model_path_obj.exists():\n",
    "#     raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "\n",
    "try:\n",
    "    # It's good practice to clear session in notebooks if re-running model related code\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model = load_model(\n",
    "        model_path,\n",
    "        custom_objects={'combined_mse_cosine_loss': combined_mse_cosine_loss}\n",
    "    )\n",
    "    print(\"Model loaded successfully.\")\n",
    "    model.summary()  # Optional: print model summary\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    # In a notebook, you might want to raise the exception to stop execution\n",
    "    raise"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: models/resnet50.keras\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001B[38;5;33mFunctional\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m11\u001B[0m, \u001B[38;5;34m25\u001B[0m, \u001B[38;5;34m2048\u001B[0m)   │    \u001B[38;5;34m23,587,712\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │       \u001B[38;5;34m524,544\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m8\u001B[0m)              │         \u001B[38;5;34m2,056\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m64,073,370\u001B[0m (244.42 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,073,370</span> (244.42 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m19,979,528\u001B[0m (76.22 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,979,528</span> (76.22 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m4,134,784\u001B[0m (15.77 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,134,784</span> (15.77 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Optimizer params: \u001B[0m\u001B[38;5;34m39,959,058\u001B[0m (152.43 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,959,058</span> (152.43 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:29:51.494208Z",
     "start_time": "2025-05-16T01:29:51.491668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, (int(984 / 3), int(2385 / 3)))\n",
    "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    return image"
   ],
   "id": "a76a34562ec3c122",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:29:57.821925Z",
     "start_time": "2025-05-16T01:29:55.941882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load image\n",
    "image = load_image(new_spectrogram_path)\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking prediction...\")\n",
    "try:\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        predictions = model.predict(image)\n",
    "\n",
    "    # predictions will be a numpy array like [[feat1, feat2, ..., feat8]]\n",
    "    predicted_features = predictions[0]\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\nPredicted Audio Features for {new_spectrogram_path}:\")\n",
    "    if len(predicted_features) == len(FEATURE_NAMES):\n",
    "        for name, value in zip(FEATURE_NAMES, predicted_features):\n",
    "            print(f\"- {name}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"Warning: Number of predicted features does not match FEATURE_NAMES length.\")\n",
    "        print(\"Raw predictions:\", predicted_features)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    # In a notebook, you might want to raise the exception\n",
    "    raise"
   ],
   "id": "26daa55efdf28eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making prediction...\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "\n",
      "Predicted Audio Features for spectrogram/test_data/Bach0_30_spectrogram_win_length=2048_hop_length=512_n_fft=2048.png:\n",
      "- acousticness: 0.9076\n",
      "- instrumentalness: 0.4246\n",
      "- liveness: 0.3151\n",
      "- speechiness: 0.6977\n",
      "- danceability: 0.1984\n",
      "- energy: 0.0391\n",
      "- tempo: 0.3793\n",
      "- valence: 0.3561\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:31:00.157674Z",
     "start_time": "2025-05-16T01:31:00.152365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Configuration for the song database ---\n",
    "DATABASE_SPECTROGRAM_DIR = Path(\"spectrogram/fma_large/\") # Directory of spectrograms\n",
    "CSV_FEATURES_PATH = Path(\"data/echonest_norm.csv\") # Path to your CSV with pre-computed features\n",
    "NUM_SIMILAR_SONGS_TO_FIND = 5\n",
    "FEATURE_WEIGHTS = np.array([0.5, 1.0, 1.0, 0, 4.0, 4.0, 3.0, 2.0], dtype=np.float32)\n",
    "# acousticness, instrumentalness, liveness, speechiness, danceability, energy, tempo, valence\n",
    "# emotional song: 1, 1, 1, 0, 2, 2, 3, 3\n",
    "# hype songs: 0.5, 1, 1, 0, 3, 3, 3, 2\n",
    "# Chill 0.5, 1, 1, 0, 3, 3, 3, 2\n",
    "\n",
    "# --- Helper Functions ---\n",
    "# parse_track_id_from_filename is no longer needed if we only use the CSV\n",
    "\n",
    "def build_feature_database_from_csv(csv_file_path, feature_names_ordered_list):\n",
    "    \"\"\"\n",
    "    Builds a feature database by reading pre-computed features directly from a CSV file.\n",
    "    Concise version with fewer sanity checks.\n",
    "    \"\"\"\n",
    "    feature_db = []\n",
    "\n",
    "    try:\n",
    "        features_df = pd.read_csv(csv_file_path)\n",
    "        # Assume the first column is the track_id, rename if not already 'track_id'\n",
    "        id_column_name = features_df.columns[0]\n",
    "        if id_column_name != 'track_id':\n",
    "            features_df.rename(columns={id_column_name: 'track_id'}, inplace=True)\n",
    "        # features_df.set_index('track_id', inplace=True) # No longer setting index, will iterate rows\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing CSV file {csv_file_path}: {e}\")\n",
    "        return feature_db # Return empty if CSV loading fails\n",
    "\n",
    "    print(f\"Building database directly from CSV: {csv_file_path}\")\n",
    "    # Iterate over rows of the DataFrame\n",
    "    for index, row in tqdm(features_df.iterrows(), total=features_df.shape[0], desc=\"Building database from CSV rows\"):\n",
    "        track_id = row['track_id'] # Get track_id from the row\n",
    "        try:\n",
    "            # Select features from the row IN THE ORDER SPECIFIED BY feature_names_ordered_list\n",
    "            feature_vector = row[feature_names_ordered_list].values.astype(np.float32)\n",
    "            # Assuming feature_vector will have correct length if all columns in feature_names_ordered_list exist\n",
    "\n",
    "            # Store the track_id (or other identifier from CSV) and its features\n",
    "            # Using track_id as the identifier now, not a file path.\n",
    "            feature_db.append((track_id, feature_vector))\n",
    "        except (KeyError, ValueError, TypeError) as e:\n",
    "            print(f\"Skipping track_id {track_id} due to error: {e}\")\n",
    "            # Skip this track if features can't be correctly extracted or cast\n",
    "            continue\n",
    "\n",
    "    print(f\"Feature database built with {len(feature_db)} songs from CSV.\")\n",
    "    return feature_db\n",
    "\n",
    "def weighted_euclidean_distance(vec1, vec2, weights):\n",
    "    \"\"\"\n",
    "    Computes the weighted Euclidean distance between two vectors.\n",
    "    Args:\n",
    "        vec1 (np.array): First feature vector.\n",
    "        vec2 (np.array): Second feature vector.\n",
    "        weights (np.array): Array of weights, one for each feature.\n",
    "                            Must have the same length as vec1 and vec2.\n",
    "    Returns:\n",
    "        float: The weighted Euclidean distance.\n",
    "    \"\"\"\n",
    "    if len(vec1) != len(weights) or len(vec2) != len(weights):\n",
    "        raise ValueError(\"Vectors and weights must have the same length.\")\n",
    "    # Element-wise multiplication of squared differences by weights\n",
    "    return np.sqrt(np.sum(weights * (np.array(vec1) - np.array(vec2))**2))\n",
    "\n",
    "def find_k_nearest_neighbors(input_song_features, db_features_list, k=5):\n",
    "    \"\"\"\n",
    "    Finds the k most similar songs from the db_features_list to the input_song_features.\n",
    "    \"\"\"\n",
    "    if not db_features_list: # Keep this check as it's fundamental\n",
    "        return []\n",
    "    distances = []\n",
    "    # song_id here will now be the track_id from the CSV\n",
    "    for song_id, feature_vec in tqdm(db_features_list, desc=f\"Finding {k} nearest neighbors\"):\n",
    "        dist = weighted_euclidean_distance(input_song_features, feature_vec, FEATURE_WEIGHTS)\n",
    "        distances.append((song_id, dist))\n",
    "    distances.sort(key=lambda item: item[1])\n",
    "    return distances[:k]"
   ],
   "id": "7b510586703bb8fb",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:31:14.138780Z",
     "start_time": "2025-05-16T01:31:12.735894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Main Logic for Similarity Search ---\n",
    "\n",
    "# The following variables are assumed to be defined in your notebook from previous cells:\n",
    "# - predicted_features: Numpy array of features for the INPUT song (from Keras model).\n",
    "# - new_spectrogram_path: Path (string or Path object) to the INPUT song's spectrogram (used for display).\n",
    "# - FEATURE_NAMES: List of 8 feature names in the order your Keras model outputs them.\n",
    "#   (This is now also used by build_feature_database_from_csv)\n",
    "\n",
    "# The Keras 'model' object and 'model_path' string are no longer directly used by\n",
    "# this specific cell's database building logic, but 'model' was used to get 'predicted_features'.\n",
    "\n",
    "# Assuming FEATURE_NAMES is correctly defined and available from previous cells.\n",
    "song_feature_database = build_feature_database_from_csv(\n",
    "    CSV_FEATURES_PATH,\n",
    "    FEATURE_NAMES # Pass the ordered list of feature names\n",
    ")"
   ],
   "id": "88b11be4dd1e2540",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building database directly from CSV: data/echonest_norm.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building database from CSV rows: 100%|██████████| 13131/13131 [00:01<00:00, 9509.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature database built with 13131 songs from CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:31:34.758701Z",
     "start_time": "2025-05-16T01:31:34.713610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not song_feature_database:\n",
    "    print(\"Cannot perform similarity search because the feature database is empty or could not be built from CSV.\")\n",
    "else:\n",
    "    display_path_for_input_song = str(new_spectrogram_path) # Used for display only\n",
    "\n",
    "    print(f\"\\nFinding {NUM_SIMILAR_SONGS_TO_FIND} songs most similar to '{display_path_for_input_song}'...\")\n",
    "    # The note about matching spectrogram filename is less relevant now, as matching is based on CSV content.\n",
    "    # However, if your input song's track_id (derived from its filename) happens to be in the CSV,\n",
    "    # it might still match itself if its features are identical.\n",
    "    print(\"Note: The input song itself may appear in results if its features are identical to an entry in the CSV.\")\n",
    "\n",
    "    similar_songs = find_k_nearest_neighbors(\n",
    "        predicted_features,\n",
    "        song_feature_database,\n",
    "        k=NUM_SIMILAR_SONGS_TO_FIND\n",
    "    )\n",
    "\n",
    "    if similar_songs:\n",
    "        print(f\"\\nTop {len(similar_songs)} similar songs (Track IDs from CSV):\")\n",
    "        for i, (song_id, dist) in enumerate(similar_songs): # song_id is now the track_id\n",
    "            print(f\"{i+1}. Track ID: {int(song_id)} (Distance: {dist:.4f})\")\n",
    "    else:\n",
    "        print(\"No similar songs found (this usually means the database was empty or no matches were found).\")\n",
    "\n"
   ],
   "id": "75ef59d0dae1e069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finding 5 songs most similar to 'spectrogram/test_data/Bach0_30_spectrogram_win_length=2048_hop_length=512_n_fft=2048.png'...\n",
      "Note: The input song itself may appear in results if its features are identical to an entry in the CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding 5 nearest neighbors: 100%|██████████| 13131/13131 [00:00<00:00, 343828.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 similar songs (Track IDs from CSV):\n",
      "1. Track ID: 5041 (Distance: 0.2790)\n",
      "2. Track ID: 48404 (Distance: 0.3740)\n",
      "3. Track ID: 32302 (Distance: 0.3751)\n",
      "4. Track ID: 31339 (Distance: 0.3756)\n",
      "5. Track ID: 5044 (Distance: 0.3832)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T01:20:30.582702Z",
     "start_time": "2025-05-16T01:20:30.581120Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3bae8da9a8f99ad0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
